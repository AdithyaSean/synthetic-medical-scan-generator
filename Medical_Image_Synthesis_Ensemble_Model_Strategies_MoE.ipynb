{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wmrXn07TmIJjM6VwTLYur4TTp4gmP_H2",
      "authorship_tag": "ABX9TyOxdXFy7Zhz13GPZpp8CtUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdithyaSean/synthetic-medical-scan-generator/blob/main/Medical_Image_Synthesis_Ensemble_Model_Strategies_MoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a comprehensive guide to developing a medical image synthesis system using ensemble models, addressing your specific questions with insights from recent research.\n",
        "\n",
        "### Ensemble Model Architectures\n",
        "\n",
        "Ensemble models are a powerful approach for your multi-modal synthesis task, as they can improve performance, robustness, and generalizability by combining the strengths of multiple models. Here's a breakdown of suitable ensemble strategies:\n",
        "\n",
        "*   **Stacking:** This involves training a meta-model to combine the outputs of several base generative models. For instance, you could use a convolutional neural network (CNN) to learn how to best combine the outputs of a Generative Adversarial Network (GAN), a Variational Autoencoder (VAE), and a diffusion model to produce a final, high-fidelity image. One study successfully used a stacking ensemble of a VGG-19 network and a Siamese neural network for medical image fusion.\n",
        "*   **Mixture of Experts (MoE):** In an MoE architecture, you would train multiple \"expert\" generative models, each specializing in a particular modality (e.g., one expert for MRI, one for CT, and one for X-ray). A \"gating network\" would then learn to determine which expert (or combination of experts) to use for a given input condition. This is highly suitable for your conditional synthesis task.\n",
        "*   **Bagging and Boosting:** While less common for generative tasks, these techniques could be adapted. For instance, in a bagging-like approach, you could train multiple generative models on different subsets of your training data and average their outputs.\n",
        "\n",
        "**Promising Combinations of Generative Models:**\n",
        "\n",
        "Recent research has shown great promise in using state-of-the-art generative models for medical imaging:\n",
        "\n",
        "*   **Diffusion Models:** These models have emerged as the leading approach for generating high-quality, diverse, and anatomically coherent medical images, often outperforming GANs in terms of training stability and image fidelity.\n",
        "*   **Generative Adversarial Networks (GANs):** GANs, particularly conditional GANs (cGANs), are a powerful tool for image-to-image translation and synthesis. They can be conditioned on various inputs, including image modality, to generate specific types of scans.\n",
        "*   **Vision Transformers (ViT):** For capturing long-range spatial relationships in medical images, ViTs can be combined with CNNs in an ensemble to enhance performance.\n",
        "\n",
        "A recommended approach would be to create a stacking ensemble that combines the outputs of a conditional diffusion model and a conditional GAN. This would leverage the strengths of both architectures – the high-fidelity output of the diffusion model and the sharp details often produced by GANs.\n",
        "\n",
        "### Conditional Generation Techniques\n",
        "\n",
        "Robustly conditioning the generative process on the input scan type is crucial. Here are some best practices:\n",
        "\n",
        "*   **Concatenation-Based Conditioning:** A simple and effective method is to encode the modality information as a one-hot vector (e.g., for MRI, for CT, for X-ray) and concatenate it with the input noise vector or the image representation at each step of the generative process. This technique has been successfully used in conditional diffusion models for 3D medical image synthesis.\n",
        "*   **Semantic Mask Conditioning:** For more fine-grained control, you can condition the model on a semantic segmentation mask. This allows you to not only specify the modality but also the desired anatomical structures and pathologies. For example, you could provide a mask of a brain with a tumor and have the model generate a realistic MRI of that brain.\n",
        "*   **Cross-Attention Mechanisms:** In Transformer-based architectures like ViTs and some diffusion models, you can use cross-attention to condition the generation process on the encoded modality information. This allows the model to learn more complex relationships between the condition and the generated image.\n",
        "\n",
        "### Type Identification/Classification\n",
        "\n",
        "For identifying the input scan type, you have two main options:\n",
        "\n",
        "1.  **Separate Pre-processing Step:** This involves training a dedicated, high-performance image classification model to first identify the modality of the input scan. This model can be an ensemble of CNNs, ViTs, or a combination of both to achieve high accuracy. This approach is modular and allows you to use a highly specialized classification model.\n",
        "2.  **Integrated Pipeline:** You can integrate the classification task into the generative pipeline. For instance, the encoder of your generative model could be trained to produce a latent representation that is not only used for generation but also fed into a small classification head to predict the modality. This can be more efficient but may require careful balancing of the generation and classification losses.\n",
        "\n",
        "For robustness, especially with noisy or varied datasets, an ensemble-based classifier is recommended. One approach is to use a two-stage selective ensemble of CNN branches, which has been shown to mitigate overfitting and the vanishing gradient problem.\n",
        "\n",
        "### Dataset Considerations and Augmentation\n",
        "\n",
        "A high-quality, diverse, and representative dataset is the foundation of your system.\n",
        "\n",
        "**Dataset Curation:**\n",
        "\n",
        "*   **Diversity:** Your dataset should include images from a wide range of patients, pathologies, and acquisition settings to ensure your model generalizes well.\n",
        "*   **Data Quality:** The images should be of high quality, with accurate and consistent annotations.\n",
        "*   **Data Balance:** Ensure a balanced representation of each modality to prevent the model from being biased towards the most frequent class.\n",
        "\n",
        "**Advanced Data Augmentation:**\n",
        "\n",
        "Beyond traditional augmentation techniques like rotation, flipping, and scaling, consider these advanced methods:\n",
        "\n",
        "*   **Generative Data Augmentation:** Use a pre-trained generative model (e.g., a GAN or diffusion model) to synthesize additional training data. This is particularly useful for augmenting underrepresented classes.\n",
        "*   **Learned Transformations:** Instead of random transformations, learn the transformations from the data itself. This can involve learning spatial deformation fields and intensity changes to create more realistic augmentations.\n",
        "*   **Semantic Data Augmentation (SDA):** This involves manipulating the latent space representations of images to create semantic variations while preserving the label.\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "To assess the clinical utility and realism of your synthetic scans, you need to go beyond standard image quality metrics.\n",
        "\n",
        "*   **Downstream Task Evaluation:** The most crucial evaluation is to assess the utility of your synthetic data for a downstream clinical task. For example, train a segmentation or classification model on your synthetic data and evaluate its performance on a real test set using metrics like the Dice Similarity Coefficient (DSC).\n",
        "*   **Visual Turing Test:** Have clinical experts try to distinguish between your synthetic images and real ones. This provides a qualitative assessment of realism.\n",
        "*   **Domain-Specific Metrics:** Instead of using FID with a feature extractor trained on natural images, use a feature extractor pre-trained on a large medical image dataset. The Fréchet MedicalNet Distance (FMD) is a good example of this.\n",
        "*   **Standard Metrics:** Continue to use standard metrics like Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Fréchet Inception Distance (FID) as a baseline, but be aware of their limitations in the medical context.\n",
        "\n",
        "### Challenges and Mitigation Strategies\n",
        "\n",
        "*   **Mode Collapse and Memorization:**\n",
        "    *   **Challenge:** The model may generate a limited variety of images or simply copy training examples.\n",
        "    *   **Mitigation:** Diffusion models are generally less prone to mode collapse than GANs. To avoid memorization, use large and diverse datasets, and employ regularization techniques.\n",
        "*   **Anatomical Inconsistency and Artifacts:**\n",
        "    *   **Challenge:** The model may generate anatomically incorrect images or introduce artifacts.\n",
        "    *   **Mitigation:** Conditioning on semantic masks can help enforce anatomical consistency. Using multi-scale discriminators in GANs can help the model learn local details and reduce artifacts.\n",
        "*   **Data Scarcity:**\n",
        "    *   **Challenge:** Limited access to large, annotated medical datasets.\n",
        "    *   **Mitigation:** Use data augmentation techniques, especially generative data augmentation, to expand your dataset. Federated learning can also be used to train on data from multiple institutions without compromising privacy.\n",
        "\n",
        "### Computational Resources\n",
        "\n",
        "Training complex ensemble models for high-resolution medical image generation is computationally intensive.\n",
        "\n",
        "*   **GPU:** High-end GPUs with large VRAM (e.g., NVIDIA A100 or H100) are essential for training these models in a reasonable timeframe.\n",
        "*   **Memory:** The memory requirements will depend on the model complexity, image resolution (especially for 3D images), and batch size. Expect to need a significant amount of RAM and VRAM.\n",
        "*   **Training Time:** Training can take anywhere from several days to weeks, depending on the size of your dataset and the complexity of your model. One study noted that a DCGAN model took 4 to 7 hours to train, while more complex models like CycleGAN took 7 to 10 hours on the same datasets.\n",
        "*   **Federated Learning:** This can distribute the computational load across multiple institutions, but it requires a robust infrastructure for communication and model aggregation."
      ],
      "metadata": {
        "id": "aloWdOm1YigT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a great question that dives into the practical application of advanced deep learning. While creating a full-fledged, production-ready ensemble model for medical image synthesis is a significant undertaking, I can provide you with a detailed Python notebook structure that serves as a starting point. This notebook will focus on a core component: training a Conditional Deep Convolutional Generative Adversarial Network (cDCGAN) to generate medical images of a specific type. You can build upon this foundation to create a more complex ensemble system.\n",
        "\n",
        "This guide is inspired by several excellent resources and tutorials on GANs and medical imaging.[1][2][3] It will walk you through the process step-by-step, explaining each part of the code. For more advanced applications, including 3D medical image synthesis and readily available tools, you might find the MONAI framework by Project MONAI to be very useful.[4]\n",
        "\n",
        "Python Notebook: Conditional GAN for Medical Image Synthesis\n",
        "\n",
        "This notebook demonstrates how to train a Conditional GAN (cGAN) to generate 2D medical images (e.g., X-rays) based on a class condition. We'll use the popular PyTorch library. You can adapt this notebook to use different medical imaging datasets and modalities."
      ],
      "metadata": {
        "id": "0GuQZ8HRSwjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports\n",
        "\n",
        "First, let's install the necessary libraries and import them. This notebook is designed to be run in a Google Colab environment to take advantage of their free GPU resources.[5]"
      ],
      "metadata": {
        "id": "9GjGvkuCSn_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision torcheval\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "fMN1BarSTCmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "72cb794a-e54e-474d-8f2a-653e8bae91fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torcheval-0.0.7\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import drive, files"
      ],
      "metadata": {
        "id": "UbJwfAJXTPc0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset and DataLoader\n",
        "\n",
        "For this example, we'll use a public medical imaging dataset. A good choice is the Chest X-Ray Images (Pneumonia) dataset from Kaggle. You will need to download it and place it in a known directory.\n",
        "\n",
        "This notebook will assume you have a directory structure like this:\n",
        "\n",
        "```\n",
        "/kaggle_data/\n",
        "    xray_dataset/\n",
        "        chest_dataset/\n",
        "            chest_xray/\n",
        "                train/\n",
        "                    NORMAL/\n",
        "                        ... (normal x-ray images)\n",
        "                    PNEUMONIA/\n",
        "                        ... (pneumonia x-ray images)\n",
        "```\n",
        "\n",
        "We will create a custom dataset class to load the images and their corresponding labels (0 for NORMAL, 1 for PNEUMONIA)."
      ],
      "metadata": {
        "id": "jdqXU9MSTofQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  HELPER FUNCTION TO RUN SHELL COMMANDS\n",
        "# ==============================================================================\n",
        "def run_command(command):\n",
        "    \"\"\"A helper function to run shell commands and print their output.\"\"\"\n",
        "    try:\n",
        "        print(f\"Executing: {command}\")\n",
        "        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
        "        if result.stdout:\n",
        "            print(\"Output:\\n\", result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"Error output:\\n\", result.stderr) # Kaggle API often prints to stderr\n",
        "        print(\"Command executed successfully!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing command: {command}\")\n",
        "        print(f\"Return code: {e.returncode}\")\n",
        "        if e.stdout:\n",
        "            print(f\"Output: \\n{e.stdout}\")\n",
        "        if e.stderr:\n",
        "            print(f\"Error output: \\n{e.stderr}\")"
      ],
      "metadata": {
        "id": "nnBW_EmojFTq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  GOOGLE DRIVE MOUNT\n",
        "# ==============================================================================\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    print(\"Proceeding without Google Drive, checkpoints will only be saved locally.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmBeqOmdjKm7",
        "outputId": "c117de2c-a744-45aa-c41a-2fb65ee7cdbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  DATASET DOWNLOAD METHODS (MedMNIST & Kaggle)\n",
        "# ==============================================================================\n",
        "# --- METHOD 1: MedMNIST v2 ---\n",
        "def download_medmnist_datasets():\n",
        "    print(\"=\"*60)\n",
        "    print(\"METHOD 1: DOWNLOADING DATASETS WITH MedMNIST\")\n",
        "    print(\"=\"*60)\n",
        "    run_command(\"pip install --quiet medmnist\")\n",
        "    try:\n",
        "        from medmnist import ChestMNIST, OrganMNISTAxial, BrainMNIST\n",
        "        print(\"\\n--- Downloading ChestMNIST (X-Ray) ---\")\n",
        "        ChestMNIST(split='train', download=True, root='./medmnist_data/')\n",
        "        ChestMNIST(split='test', download=True, root='./medmnist_data/')\n",
        "        print(\"ChestMNIST downloaded successfully to ./medmnist_data/\")\n",
        "\n",
        "        print(\"\\n--- Downloading OrganMNISTAxial (CT) ---\")\n",
        "        OrganMNISTAxial(split='train', download=True, root='./medmnist_data/')\n",
        "        OrganMNISTAxial(split='test', download=True, root='./medmnist_data/')\n",
        "        print(\"OrganMNISTAxial downloaded successfully to ./medmnist_data/\")\n",
        "\n",
        "        print(\"\\n--- Downloading BrainMNIST (MRI) ---\")\n",
        "        BrainMNIST(split='train', download=True, root='./medmnist_data/')\n",
        "        BrainMNIST(split='test', download=True, root='./medmnist_data/')\n",
        "        print(\"BrainMNIST downloaded successfully to ./medmnist_data/\")\n",
        "        print(\"\\nAll selected MedMNIST datasets downloaded.\")\n",
        "    except ImportError as e:\n",
        "        print(f\"Could not import from medmnist. Please ensure it is installed correctly. Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during MedMNIST download: {e}\")\n",
        "\n",
        "# --- METHOD 2: KAGGLE ---\n",
        "KAGGLE_DATA_DIR = \"./kaggle_data\" # Base directory for Kaggle datasets\n",
        "\n",
        "def download_kaggle_dataset(dataset_slug, unzip_name):\n",
        "    print(f\"\\n--- Downloading {dataset_slug} from Kaggle ---\")\n",
        "    run_command(f\"kaggle datasets download -d {dataset_slug} -p {KAGGLE_DATA_DIR} --force\") # --force to overwrite if exists\n",
        "    zip_file_name = dataset_slug.split('/')[-1] + \".zip\"\n",
        "    zip_path = os.path.join(KAGGLE_DATA_DIR, zip_file_name)\n",
        "    unzip_target_path = os.path.join(KAGGLE_DATA_DIR, unzip_name)\n",
        "    run_command(f\"mkdir -p {unzip_target_path}\")\n",
        "    run_command(f\"unzip -q -o {zip_path} -d {unzip_target_path}\") # -o to overwrite\n",
        "    run_command(f\"rm {zip_path}\")\n",
        "    print(f\"{dataset_slug} dataset ready at: {unzip_target_path}\")\n",
        "    return unzip_target_path\n",
        "\n",
        "def setup_kaggle_api():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"METHOD 2: DOWNLOADING DATASETS FROM KAGGLE\")\n",
        "    print(\"=\"*60)\n",
        "    run_command(\"pip install --quiet kaggle\")\n",
        "    try:\n",
        "        print(\"\\nPlease upload your 'kaggle.json' file (if not already configured):\")\n",
        "        # Check if kaggle.json already exists to avoid re-uploading if kernel restarts\n",
        "        if not os.path.exists(\"~/.kaggle/kaggle.json\") and not os.path.exists(\"kaggle.json\"):\n",
        "             files.upload() # This will prompt for upload\n",
        "        else:\n",
        "            print(\"'kaggle.json' or ~/.kaggle/kaggle.json seems to exist. Skipping upload.\")\n",
        "\n",
        "        if os.path.exists(\"kaggle.json\"): # If uploaded to current dir\n",
        "            run_command(\"mkdir -p ~/.kaggle\")\n",
        "            run_command(\"cp kaggle.json ~/.kaggle/\")\n",
        "            run_command(\"chmod 600 ~/.kaggle/kaggle.json\")\n",
        "        elif not os.path.exists(\"~/.kaggle/kaggle.json\"):\n",
        "            print(\"Kaggle API token not found. Please ensure kaggle.json is uploaded or placed in ~/.kaggle/\")\n",
        "            return False\n",
        "        print(\"\\nKaggle API configured successfully (or was already configured).\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during Kaggle setup: {e}\")\n",
        "        print(\"If running locally, please place your 'kaggle.json' file in '~/.kaggle/' manually.\")\n",
        "        return False"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0BSgoej0LU4Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  DATASET CONFIGURATION\n",
        "# ==============================================================================\n",
        "# Define configurations for different datasets\n",
        "# 'image_data_path_relative': Path relative to the dataset's base_path where class folders or images are.\n",
        "# 'num_classes': For cGAN, number of distinct classes. For unconditional GAN, can be 1.\n",
        "# 'img_channels': Number of image channels (1 for grayscale, 3 for RGB).\n",
        "# 'img_size': Target image size after resizing.\n",
        "\n",
        "DATASET_CONFIGS = {\n",
        "    \"covid_ct\": {\n",
        "        \"needs_kaggle_api\": True,\n",
        "        \"kaggle_slug\": \"maedemaftouni/large-covid19-ct-slice-dataset\",\n",
        "        \"base_path_segment\": \"covid_ct_dataset\", # Folder name under KAGGLE_DATA_DIR\n",
        "        # This dataset often unzips with a nested structure.\n",
        "        # The actual image classes (e.g., '1NonCOVID', '2COVID') are typically here:\n",
        "        \"image_data_path_relative\": [\"curated_data/curated_data\", \"Large-COVID-19-CT-slice-dataset/curated_data/curated_data\"],\n",
        "        \"num_classes\": 2, # Example: COVID vs Non-COVID\n",
        "        \"img_channels\": 1, # Will be transformed to grayscale\n",
        "        \"img_size\": 64\n",
        "    },\n",
        "    \"chest_xray\": {\n",
        "        \"needs_kaggle_api\": True,\n",
        "        \"kaggle_slug\": \"paultimothymooney/chest-xray-pneumonia\",\n",
        "        \"base_path_segment\": \"xray_dataset\",\n",
        "        # Assumes training images are in 'chest_xray/train' relative to base_path_segment\n",
        "        \"image_data_path_relative\": [\"chest_xray/train\", \"chest-xray-pneumonia/train\"],\n",
        "        \"num_classes\": 2, # PNEUMONIA vs NORMAL\n",
        "        \"img_channels\": 1, # Will be transformed to grayscale\n",
        "        \"img_size\": 64\n",
        "    },\n",
        "    # Add more dataset configs here if needed\n",
        "    # \"brats_mri\": { ... }\n",
        "}\n",
        "\n",
        "# --- SELECT THE DATASET TO USE ---\n",
        "# Change this to \"chest_xray\", or other keys you add to DATASET_CONFIGS\n",
        "SELECTED_DATASET_NAME = \"covid_ct\"\n",
        "# SELECTED_DATASET_NAME = \"chest_xray\"\n",
        "\n",
        "if SELECTED_DATASET_NAME not in DATASET_CONFIGS:\n",
        "    raise ValueError(f\"Dataset '{SELECTED_DATASET_NAME}' is not configured in DATASET_CONFIGS.\")\n",
        "\n",
        "ACTIVE_DATASET_CONFIG = DATASET_CONFIGS[SELECTED_DATASET_NAME]"
      ],
      "metadata": {
        "id": "qtYXPW-Akfm-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  DOWNLOAD SELECTED DATASET (if applicable)\n",
        "# ==============================================================================\n",
        "# This block will attempt to download the selected dataset if it's from Kaggle\n",
        "# and if the Kaggle API is configured.\n",
        "\n",
        "dataset_base_path = None\n",
        "actual_image_data_root = None\n",
        "\n",
        "if ACTIVE_DATASET_CONFIG.get(\"needs_kaggle_api\", False):\n",
        "    if setup_kaggle_api():\n",
        "        dataset_base_path = download_kaggle_dataset(\n",
        "            ACTIVE_DATASET_CONFIG[\"kaggle_slug\"],\n",
        "            ACTIVE_DATASET_CONFIG[\"base_path_segment\"]\n",
        "        )\n",
        "\n",
        "        # Determine the actual image data root by checking potential relative paths\n",
        "        found_path = False\n",
        "        for rel_path in ACTIVE_DATASET_CONFIG[\"image_data_path_relative\"]:\n",
        "            potential_path = os.path.join(dataset_base_path, rel_path)\n",
        "            if os.path.exists(potential_path) and os.path.isdir(potential_path):\n",
        "                actual_image_data_root = potential_path\n",
        "                print(f\"Image data found at: {actual_image_data_root}\")\n",
        "                found_path = True\n",
        "                break\n",
        "        if not found_path:\n",
        "            print(f\"ERROR: Could not find image data directory for {SELECTED_DATASET_NAME} at expected relative paths within {dataset_base_path}.\")\n",
        "            print(f\"Please check the 'image_data_path_relative' in DATASET_CONFIGS for {SELECTED_DATASET_NAME} and the unzipped Kaggle dataset structure.\")\n",
        "            # Optionally, list the contents of dataset_base_path to help debug\n",
        "            print(f\"Contents of {dataset_base_path}:\")\n",
        "            run_command(f\"ls -lR {dataset_base_path}\")\n",
        "            # Set actual_image_data_root to None to prevent dataloader creation if path is not found\n",
        "            actual_image_data_root = None\n",
        "    else:\n",
        "        print(\"Kaggle API setup failed. Cannot download Kaggle dataset.\")\n",
        "# Add MedMNIST download trigger if a MedMNIST dataset is selected (not implemented in this example config)\n",
        "# elif ACTIVE_DATASET_CONFIG.get(\"source\") == \"medmnist\":\n",
        "#     download_medmnist_datasets()\n",
        "#     actual_image_data_root = os.path.join(\"./medmnist_data\", ACTIVE_DATASET_CONFIG[\"medmnist_name\"], \"train\") # Adjust as needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1xVa7-MeknCx",
        "outputId": "09cf73b1-736d-4536-c436-8c5d5a79f902"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METHOD 2: DOWNLOADING DATASETS FROM KAGGLE\n",
            "============================================================\n",
            "Executing: pip install --quiet kaggle\n",
            "Command executed successfully!\n",
            "\n",
            "Please upload your 'kaggle.json' file (if not already configured):\n",
            "'kaggle.json' or ~/.kaggle/kaggle.json seems to exist. Skipping upload.\n",
            "Executing: mkdir -p ~/.kaggle\n",
            "Command executed successfully!\n",
            "Executing: cp kaggle.json ~/.kaggle/\n",
            "Command executed successfully!\n",
            "Executing: chmod 600 ~/.kaggle/kaggle.json\n",
            "Command executed successfully!\n",
            "\n",
            "Kaggle API configured successfully (or was already configured).\n",
            "\n",
            "--- Downloading maedemaftouni/large-covid19-ct-slice-dataset from Kaggle ---\n",
            "Executing: kaggle datasets download -d maedemaftouni/large-covid19-ct-slice-dataset -p ./kaggle_data --force\n",
            "Output:\n",
            " Dataset URL: https://www.kaggle.com/datasets/maedemaftouni/large-covid19-ct-slice-dataset\n",
            "License(s): other\n",
            "Downloading large-covid19-ct-slice-dataset.zip to ./kaggle_data\n",
            "\n",
            "\n",
            "Error output:\n",
            " \n",
            "  0%|          | 0.00/2.06G [00:00<?, ?B/s]\n",
            "  5%|▍         | 96.0M/2.06G [00:00<00:02, 996MB/s]\n",
            "  9%|▉         | 192M/2.06G [00:00<00:02, 781MB/s] \n",
            " 13%|█▎        | 270M/2.06G [00:02<00:26, 71.5MB/s]\n",
            " 15%|█▍        | 314M/2.06G [00:03<00:21, 87.6MB/s]\n",
            " 17%|█▋        | 351M/2.06G [00:03<00:17, 103MB/s] \n",
            " 18%|█▊        | 384M/2.06G [00:03<00:15, 119MB/s]\n",
            " 20%|█▉        | 414M/2.06G [00:03<00:13, 129MB/s]\n",
            " 21%|██        | 440M/2.06G [00:03<00:13, 130MB/s]\n",
            " 22%|██▏       | 462M/2.06G [00:03<00:12, 137MB/s]\n",
            " 23%|██▎       | 482M/2.06G [00:03<00:11, 147MB/s]\n",
            " 24%|██▍       | 506M/2.06G [00:04<00:10, 163MB/s]\n",
            " 25%|██▍       | 527M/2.06G [00:04<00:09, 171MB/s]\n",
            " 26%|██▌       | 550M/2.06G [00:04<00:09, 181MB/s]\n",
            " 27%|██▋       | 571M/2.06G [00:04<00:08, 190MB/s]\n",
            " 29%|██▊       | 602M/2.06G [00:04<00:07, 222MB/s]\n",
            " 30%|██▉       | 626M/2.06G [00:04<00:07, 219MB/s]\n",
            " 31%|███▏      | 665M/2.06G [00:04<00:05, 267MB/s]\n",
            " 34%|███▍      | 714M/2.06G [00:04<00:04, 327MB/s]\n",
            " 36%|███▌      | 759M/2.06G [00:04<00:03, 362MB/s]\n",
            " 38%|███▊      | 795M/2.06G [00:05<00:04, 316MB/s]\n",
            " 39%|███▉      | 827M/2.06G [00:05<00:04, 314MB/s]\n",
            " 41%|████      | 859M/2.06G [00:05<00:04, 263MB/s]\n",
            " 42%|████▏     | 896M/2.06G [00:05<00:04, 292MB/s]\n",
            " 44%|████▍     | 928M/2.06G [00:05<00:04, 301MB/s]\n",
            " 45%|████▌     | 959M/2.06G [00:05<00:04, 280MB/s]\n",
            " 47%|████▋     | 992M/2.06G [00:05<00:03, 294MB/s]\n",
            " 48%|████▊     | 1.00G/2.06G [00:05<00:04, 275MB/s]\n",
            " 50%|████▉     | 1.03G/2.06G [00:06<00:04, 269MB/s]\n",
            " 51%|█████     | 1.05G/2.06G [00:06<00:03, 274MB/s]\n",
            " 52%|█████▏    | 1.08G/2.06G [00:06<00:04, 236MB/s]\n",
            " 53%|█████▎    | 1.10G/2.06G [00:06<00:04, 229MB/s]\n",
            " 55%|█████▍    | 1.12G/2.06G [00:06<00:04, 217MB/s]\n",
            " 56%|█████▌    | 1.15G/2.06G [00:06<00:04, 217MB/s]\n",
            " 57%|█████▋    | 1.17G/2.06G [00:06<00:05, 182MB/s]\n",
            " 58%|█████▊    | 1.20G/2.06G [00:06<00:04, 223MB/s]\n",
            " 59%|█████▉    | 1.22G/2.06G [00:07<00:04, 210MB/s]\n",
            " 60%|██████    | 1.25G/2.06G [00:07<00:04, 213MB/s]\n",
            " 62%|██████▏   | 1.27G/2.06G [00:07<00:03, 233MB/s]\n",
            " 63%|██████▎   | 1.30G/2.06G [00:07<00:03, 261MB/s]\n",
            " 65%|██████▍   | 1.33G/2.06G [00:07<00:02, 274MB/s]\n",
            " 66%|██████▌   | 1.36G/2.06G [00:07<00:03, 250MB/s]\n",
            " 67%|██████▋   | 1.38G/2.06G [00:07<00:03, 211MB/s]\n",
            " 68%|██████▊   | 1.41G/2.06G [00:07<00:03, 211MB/s]\n",
            " 69%|██████▉   | 1.43G/2.06G [00:08<00:03, 209MB/s]\n",
            " 70%|███████   | 1.45G/2.06G [00:08<00:03, 203MB/s]\n",
            " 71%|███████▏  | 1.47G/2.06G [00:08<00:02, 212MB/s]\n",
            " 72%|███████▏  | 1.49G/2.06G [00:08<00:02, 211MB/s]\n",
            " 73%|███████▎  | 1.51G/2.06G [00:08<00:02, 213MB/s]\n",
            " 74%|███████▍  | 1.53G/2.06G [00:08<00:02, 193MB/s]\n",
            " 75%|███████▌  | 1.55G/2.06G [00:08<00:03, 153MB/s]\n",
            " 76%|███████▌  | 1.57G/2.06G [00:08<00:03, 167MB/s]\n",
            " 77%|███████▋  | 1.59G/2.06G [00:09<00:02, 180MB/s]\n",
            " 78%|███████▊  | 1.61G/2.06G [00:09<00:02, 189MB/s]\n",
            " 79%|███████▉  | 1.63G/2.06G [00:09<00:02, 193MB/s]\n",
            " 80%|████████  | 1.65G/2.06G [00:09<00:02, 191MB/s]\n",
            " 81%|████████  | 1.67G/2.06G [00:09<00:02, 157MB/s]\n",
            " 82%|████████▏ | 1.69G/2.06G [00:09<00:02, 167MB/s]\n",
            " 83%|████████▎ | 1.71G/2.06G [00:10<00:04, 92.8MB/s]\n",
            " 84%|████████▎ | 1.72G/2.06G [00:10<00:03, 109MB/s] \n",
            " 84%|████████▍ | 1.74G/2.06G [00:10<00:03, 112MB/s]\n",
            " 85%|████████▌ | 1.75G/2.06G [00:10<00:02, 121MB/s]\n",
            " 86%|████████▌ | 1.77G/2.06G [00:10<00:02, 124MB/s]\n",
            " 87%|████████▋ | 1.79G/2.06G [00:10<00:02, 142MB/s]\n",
            " 87%|████████▋ | 1.80G/2.06G [00:10<00:01, 151MB/s]\n",
            " 88%|████████▊ | 1.82G/2.06G [00:10<00:01, 162MB/s]\n",
            " 89%|████████▉ | 1.84G/2.06G [00:10<00:01, 168MB/s]\n",
            " 90%|█████████ | 1.86G/2.06G [00:11<00:01, 183MB/s]\n",
            " 91%|█████████ | 1.88G/2.06G [00:11<00:01, 187MB/s]\n",
            " 92%|█████████▏| 1.90G/2.06G [00:11<00:00, 191MB/s]\n",
            " 93%|█████████▎| 1.92G/2.06G [00:11<00:00, 193MB/s]\n",
            " 94%|█████████▍| 1.94G/2.06G [00:11<00:00, 194MB/s]\n",
            " 95%|█████████▍| 1.95G/2.06G [00:11<00:00, 194MB/s]\n",
            " 96%|█████████▌| 1.98G/2.06G [00:11<00:00, 212MB/s]\n",
            " 97%|█████████▋| 2.00G/2.06G [00:11<00:00, 207MB/s]\n",
            " 98%|█████████▊| 2.02G/2.06G [00:11<00:00, 204MB/s]\n",
            " 99%|█████████▉| 2.04G/2.06G [00:11<00:00, 192MB/s]\n",
            "100%|█████████▉| 2.06G/2.06G [00:12<00:00, 179MB/s]\n",
            "100%|██████████| 2.06G/2.06G [00:12<00:00, 182MB/s]\n",
            "\n",
            "Command executed successfully!\n",
            "Executing: mkdir -p ./kaggle_data/covid_ct_dataset\n",
            "Command executed successfully!\n",
            "Executing: unzip -q -o ./kaggle_data/large-covid19-ct-slice-dataset.zip -d ./kaggle_data/covid_ct_dataset\n",
            "Command executed successfully!\n",
            "Executing: rm ./kaggle_data/large-covid19-ct-slice-dataset.zip\n",
            "Command executed successfully!\n",
            "maedemaftouni/large-covid19-ct-slice-dataset dataset ready at: ./kaggle_data/covid_ct_dataset\n",
            "Image data found at: ./kaggle_data/covid_ct_dataset/curated_data/curated_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  CUSTOM DATASET CLASSES\n",
        "# ==============================================================================\n",
        "# Generic Dataset Class for image folders (can be used for ChestXRay, COVID-CT if structured with class subfolders)\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, expected_channels=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the image class folders (e.g., 'train' folder containing 'NORMAL' and 'PNEUMONIA').\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            expected_channels (int): 1 for grayscale, 3 for RGB. For .convert()\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "        self.class_names = []\n",
        "\n",
        "        if not os.path.exists(root_dir):\n",
        "            print(f\"Error: Root directory for dataset not found at {root_dir}\")\n",
        "            return\n",
        "\n",
        "        # Assuming structure: root_dir/class_name/image.png\n",
        "        sorted_classes = sorted(os.listdir(root_dir)) # Sort for consistent label assignment\n",
        "        for i, class_name in enumerate(sorted_classes):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                self.class_names.append(class_name)\n",
        "                for img_name in os.listdir(class_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):\n",
        "                        self.image_files.append(os.path.join(class_path, img_name))\n",
        "                        self.labels.append(i) # Assign label based on folder index\n",
        "\n",
        "        if not self.image_files:\n",
        "            print(f\"Warning: No image files found in {root_dir} or its subdirectories.\")\n",
        "        else:\n",
        "            print(f\"Found {len(self.image_files)} images in {len(self.class_names)} classes from {root_dir}.\")\n",
        "            print(f\"Classes found: {self.class_names}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.image_files[idx]\n",
        "        try:\n",
        "            # Ensure consistent number of channels based on what the transform expects\n",
        "            # For this GAN, we usually convert to RGB first, then Grayscale in transform if needed.\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a dummy image and label or raise error\n",
        "            dummy_shape = (ACTIVE_DATASET_CONFIG['img_size'], ACTIVE_DATASET_CONFIG['img_size'])\n",
        "            if ACTIVE_DATASET_CONFIG['img_channels'] == 1:\n",
        "                return torch.zeros((1, *dummy_shape)), torch.tensor(0, dtype=torch.long)\n",
        "            else:\n",
        "                return torch.zeros((3, *dummy_shape)), torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Note: BraTS2020Dataset is more complex due to NIfTI files and 3D data.\n",
        "# The placeholder is kept here, but a full implementation is beyond this scope.\n",
        "# class BraTS2020Dataset(Dataset): ..."
      ],
      "metadata": {
        "id": "LsT522lSkwg5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  DATA TRANSFORMATION AND DATALOADER CREATION\n",
        "# ==============================================================================\n",
        "# These should be available from your ACTIVE_DATASET_CONFIG\n",
        "IMG_SIZE = ACTIVE_DATASET_CONFIG['img_size']\n",
        "IMG_CHANNELS = ACTIVE_DATASET_CONFIG['img_channels']\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.Grayscale(num_output_channels=IMG_CHANNELS), # Ensure correct channels (1 for grayscale, 3 for RGB)\n",
        "    transforms.ToTensor(), # Converts PIL image or numpy.ndarray to tensor and scales to [0, 1]\n",
        "    transforms.Normalize((0.5,) * IMG_CHANNELS, (0.5,) * IMG_CHANNELS) # Normalizes to [-1, 1] for each channel\n",
        "])\n",
        "\n",
        "# --- Attempt to create DataLoader for the selected dataset ---\n",
        "main_dataloader = None # Initialize to None\n",
        "\n",
        "if actual_image_data_root and os.path.exists(actual_image_data_root) and os.path.isdir(actual_image_data_root):\n",
        "    # The ImageFolderDataset class itself prints messages about found images/classes or errors during its initialization.\n",
        "    print(f\"Attempting to load dataset from: {actual_image_data_root}\")\n",
        "    selected_dataset = ImageFolderDataset(root_dir=actual_image_data_root, transform=transform)\n",
        "\n",
        "    if len(selected_dataset) > 0: # Proceed only if the dataset found some images\n",
        "        # --- Dynamically update num_classes based on dataset scan ---\n",
        "        actual_num_classes_found = len(selected_dataset.class_names)\n",
        "\n",
        "        if actual_num_classes_found > 0:\n",
        "            # Case 1: Classes (subdirectories with images) were found in the dataset directory\n",
        "            if actual_num_classes_found != ACTIVE_DATASET_CONFIG['num_classes']:\n",
        "                print(f\"INFO: Dynamically updating num_classes for '{SELECTED_DATASET_NAME}'.\")\n",
        "                print(f\"  Config had: {ACTIVE_DATASET_CONFIG['num_classes']} classes.\")\n",
        "                print(f\"  Dataset scan found: {actual_num_classes_found} classes ({selected_dataset.class_names}).\")\n",
        "                ACTIVE_DATASET_CONFIG['num_classes'] = actual_num_classes_found\n",
        "                print(f\"  ACTIVE_DATASET_CONFIG['num_classes'] is now: {ACTIVE_DATASET_CONFIG['num_classes']}.\")\n",
        "            else:\n",
        "                # Number of classes found matches the config, no update needed, but good to confirm.\n",
        "                print(f\"INFO: Confirmed num_classes for '{SELECTED_DATASET_NAME}'.\")\n",
        "                print(f\"  Config and dataset scan both indicate: {actual_num_classes_found} classes ({selected_dataset.class_names}).\")\n",
        "\n",
        "        elif actual_num_classes_found == 0:\n",
        "            # Case 2: No class subdirectories with images were found by ImageFolderDataset\n",
        "            # This means root_dir itself might contain images, or it's truly empty of class structures.\n",
        "            # ImageFolderDataset as written expects class subfolders. If it finds 0 class_names,\n",
        "            # it implies no such subfolders were processed.\n",
        "            print(f\"WARNING: No class subdirectories with images were found by ImageFolderDataset in '{actual_image_data_root}'.\")\n",
        "            if ACTIVE_DATASET_CONFIG['num_classes'] > 0: # If config expected classes\n",
        "                print(f\"  Config for '{SELECTED_DATASET_NAME}' expected {ACTIVE_DATASET_CONFIG['num_classes']} classes.\")\n",
        "                print(f\"  This discrepancy might lead to issues if models require conditional input based on num_classes > 0.\")\n",
        "                # Decision point: What to do?\n",
        "                # Option A: Abort if classes are strictly necessary:\n",
        "                #   raise ValueError(f\"Dataset at {actual_image_data_root} has no class subfolders, but config expects {ACTIVE_DATASET_CONFIG['num_classes']}.\")\n",
        "                # Option B: Fallback to treating it as unconditional (num_classes = 1 or effectively ignored by model)\n",
        "                #   ACTIVE_DATASET_CONFIG['num_classes'] = 1 # Or handle this in model appropriately\n",
        "                #   print(f\"  Setting num_classes to 1 as a fallback for unconditional processing.\")\n",
        "                # For now, we'll just warn and proceed with the config's num_classes.\n",
        "                # The models should be robust or this case should be handled by specific dataset logic if needed.\n",
        "                print(f\"  Proceeding with config's num_classes = {ACTIVE_DATASET_CONFIG['num_classes']}. Verify model compatibility.\")\n",
        "\n",
        "            else: # actual_num_classes_found == 0 and ACTIVE_DATASET_CONFIG['num_classes'] == 0 (or was already 0)\n",
        "                print(f\"  Config for '{SELECTED_DATASET_NAME}' also has num_classes = 0. Assuming unconditional GAN setup or this is intended.\")\n",
        "        # --- End of dynamic num_classes update ---\n",
        "\n",
        "        # Instantiate DataLoader\n",
        "        # Ensure BATCH_SIZE and NUM_WORKERS are defined (e.g., in your Hyperparameters section)\n",
        "        # Using typical values here as placeholders if they aren't globally defined for this snippet.\n",
        "        current_batch_size = 64 # Replace with your BATCH_SIZE variable if defined elsewhere\n",
        "        current_num_workers = 2  # Replace with your NUM_WORKERS variable if defined elsewhere\n",
        "\n",
        "        main_dataloader = DataLoader(\n",
        "            selected_dataset,\n",
        "            batch_size=current_batch_size,\n",
        "            shuffle=True, # Shuffle data for training\n",
        "            num_workers=current_num_workers,\n",
        "            pin_memory=True # Can speed up data transfer to GPU if available\n",
        "        )\n",
        "\n",
        "        # Report the final state after attempting DataLoader creation\n",
        "        print(f\"--- DataLoader Creation Summary for '{SELECTED_DATASET_NAME}' ---\")\n",
        "        print(f\"  Dataset path: {actual_image_data_root}\")\n",
        "        print(f\"  Total images found by Dataset class: {len(selected_dataset)}\")\n",
        "        print(f\"  Number of classes for model (from ACTIVE_DATASET_CONFIG): {ACTIVE_DATASET_CONFIG['num_classes']}\")\n",
        "        if selected_dataset.class_names:\n",
        "             print(f\"  Classes detected in dataset structure: {selected_dataset.class_names}\")\n",
        "        print(f\"  DataLoader created successfully.\")\n",
        "\n",
        "        # Optional: Display an example batch (uncomment to use for verification)\n",
        "        # print(f\"\\n--- Verifying DataLoader: Example Batch from '{SELECTED_DATASET_NAME}' ---\")\n",
        "        # try:\n",
        "        #     example_images, example_labels = next(iter(main_dataloader))\n",
        "        #     print(f\"  Batch details - Images shape: {example_images.shape}, Labels shape: {example_labels.shape}\")\n",
        "        #     print(f\"  Example labels in batch: {example_labels[:5].tolist()}...\") # Show first 5 labels\n",
        "        #     if IMG_CHANNELS == 1 and example_images.nelement() > 0:\n",
        "        #         plt.figure(figsize=(3,3))\n",
        "        #         plt.imshow(example_images[0].squeeze().cpu().numpy(), cmap='gray')\n",
        "        #         plt.title(f\"Label: {example_labels[0].item()}\")\n",
        "        #         plt.axis('off')\n",
        "        #         plt.show()\n",
        "        #     elif example_images.nelement() > 0: # For RGB\n",
        "        #          plt.figure(figsize=(3,3))\n",
        "        #          # Unnormalize for display: (data * 0.5) + 0.5\n",
        "        #          img_to_show = (example_images[0].permute(1,2,0).cpu().numpy() * 0.5) + 0.5\n",
        "        #          plt.imshow(img_to_show.clip(0,1)) # Clip to ensure valid range for imshow\n",
        "        #          plt.title(f\"Label: {example_labels[0].item()}\")\n",
        "        #          plt.axis('off')\n",
        "        #          plt.show()\n",
        "        # except StopIteration:\n",
        "        #     print(\"  ERROR: Could not retrieve an example batch (DataLoader might be empty unexpectedly).\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"  ERROR displaying example batch: {e}\")\n",
        "\n",
        "    else: # This 'else' corresponds to 'if len(selected_dataset) > 0'\n",
        "        print(f\"ERROR: Dataset for '{SELECTED_DATASET_NAME}' was initialized but found 0 images in '{actual_image_data_root}'.\")\n",
        "        print(f\"  DataLoader cannot be created with an empty dataset.\")\n",
        "        if selected_dataset.class_names: # If class folders existed but were empty of valid images\n",
        "            print(f\"  Note: Class folders were detected by ImageFolderDataset: {selected_dataset.class_names}, but they contained no loadable image files.\")\n",
        "        elif not os.listdir(actual_image_data_root):\n",
        "             print(f\"  Note: The directory '{actual_image_data_root}' appears to be empty.\")\n",
        "        else:\n",
        "            print(f\"  Note: Please check the contents of '{actual_image_data_root}' for valid image files and expected class subfolder structure.\")\n",
        "\n",
        "else: # This 'else' corresponds to 'if actual_image_data_root and os.path.exists(actual_image_data_root) ...'\n",
        "    if not actual_image_data_root:\n",
        "        print(f\"ERROR: Image data root path for '{SELECTED_DATASET_NAME}' was not determined (variable is None). DataLoader not created.\")\n",
        "    elif not os.path.exists(actual_image_data_root):\n",
        "        print(f\"ERROR: Image data root path for '{SELECTED_DATASET_NAME}' ('{actual_image_data_root}') does not exist. DataLoader not created.\")\n",
        "    elif not os.path.isdir(actual_image_data_root):\n",
        "        print(f\"ERROR: Image data root path for '{SELECTED_DATASET_NAME}' ('{actual_image_data_root}') is not a directory. DataLoader not created.\")\n",
        "\n",
        "# The rest of your script (Model Definitions, Hyperparameters & Setup, etc.)\n",
        "# will proceed. If main_dataloader is None, the training loop should have a check.\n",
        "# The num_classes used for model initialization should be taken from ACTIVE_DATASET_CONFIG['num_classes']\n",
        "# E.g., in your \"Hyperparameters & Setup\" section:\n",
        "#   num_classes_for_model = ACTIVE_DATASET_CONFIG['num_classes']\n",
        "#   generator = Generator(latent_dim, num_classes_for_model, img_shape_tuple)\n",
        "#   discriminator = BCEDiscriminator(num_classes_for_model, img_shape_tuple)"
      ],
      "metadata": {
        "id": "RL6b3ksnTrp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a4efe1-492e-4e3b-8b10-f3f01c94a1f6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from: ./kaggle_data/covid_ct_dataset/curated_data/curated_data\n",
            "Found 17104 images in 3 classes from ./kaggle_data/covid_ct_dataset/curated_data/curated_data.\n",
            "Classes found: ['1NonCOVID', '2COVID', '3CAP']\n",
            "INFO: Dynamically updating num_classes for 'covid_ct'.\n",
            "  Config had: 2 classes.\n",
            "  Dataset scan found: 3 classes (['1NonCOVID', '2COVID', '3CAP']).\n",
            "  ACTIVE_DATASET_CONFIG['num_classes'] is now: 3.\n",
            "--- DataLoader Creation Summary for 'covid_ct' ---\n",
            "  Dataset path: ./kaggle_data/covid_ct_dataset/curated_data/curated_data\n",
            "  Total images found by Dataset class: 17104\n",
            "  Number of classes for model (from ACTIVE_DATASET_CONFIG): 3\n",
            "  Classes detected in dataset structure: ['1NonCOVID', '2COVID', '3CAP']\n",
            "  DataLoader created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generator Network\n",
        "\n",
        "The Generator takes a random noise vector and a class label as input and generates an image."
      ],
      "metadata": {
        "id": "_rAo_78lUa6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  MODEL DEFINITIONS (Generator)\n",
        "# ==============================================================================\n",
        "# --- Generator ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes, img_shape_tuple): # img_shape_tuple (C, H, W)\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.img_shape_tuple = img_shape_tuple\n",
        "        self.init_size = img_shape_tuple[1] // 4 # Initial size for transposed conv.\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128 * (self.img_shape_tuple[1]//4)**2 ), # Adjusted for upsampling path\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Upsampling path (example using ConvTranspose2d)\n",
        "        # This is a common DCGAN-style generator architecture. The original was MLP-based.\n",
        "        # If sticking to MLP, the Linear layers need to output np.prod(img_shape_tuple).\n",
        "        # The original code had a simple MLP which might struggle with image generation.\n",
        "        # Let's keep the original MLP structure for now as per the user's code,\n",
        "        # but note that CNN-based generators are generally better for images.\n",
        "\n",
        "        self.mlp_model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256), # BatchNorm after Linear, before activation\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, int(np.prod(self.img_shape_tuple))),\n",
        "            nn.Tanh() # To scale output to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
        "        img_flat = self.mlp_model(gen_input)\n",
        "        img = img_flat.view(img_flat.size(0), *self.img_shape_tuple)\n",
        "        return img"
      ],
      "metadata": {
        "id": "D7MDYV1kUiqP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Discriminator Network\n",
        "\n",
        "The Discriminator takes an image and a class label as input and determines if the image is real or fake."
      ],
      "metadata": {
        "id": "SIrvehqNUoJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  MODEL DEFINITIONS (Discriminator)\n",
        "# ==============================================================================\n",
        "# --- Discriminator (for BCEWithLogitsLoss) ---\n",
        "class BCEDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, img_shape_tuple): # img_shape_tuple (C, H, W)\n",
        "        super(BCEDiscriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "        self.img_shape_tuple = img_shape_tuple\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_classes + int(np.prod(self.img_shape_tuple)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1) # Output layer (no sigmoid, BCEWithLogitsLoss handles it)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        d_in = torch.cat((img_flat, self.label_embedding(labels)), -1)\n",
        "        validity = self.model(d_in)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "QoYkRXn4U6_F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training Loop\n",
        "\n",
        "This is the core of the GAN training process. The Generator and Discriminator are trained in an adversarial manner."
      ],
      "metadata": {
        "id": "Sooj1_6QVAnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  HYPERPARAMETERS & SETUP\n",
        "# ==============================================================================\n",
        "latent_dim = 100\n",
        "num_classes = ACTIVE_DATASET_CONFIG['num_classes']\n",
        "# Image shape tuple: (channels, height, width)\n",
        "img_shape_tuple = (ACTIVE_DATASET_CONFIG['img_channels'], ACTIVE_DATASET_CONFIG['img_size'], ACTIVE_DATASET_CONFIG['img_size'])\n",
        "epochs = 100\n",
        "lr = 0.0002\n",
        "b1 = 0.5  # Adam optimizer beta1\n",
        "b2 = 0.999 # Adam optimizer beta2\n",
        "batch_size = 64 # Already used in DataLoader, ensure consistency or pass from here\n",
        "\n",
        "# --- Directories for saving ---\n",
        "LOCAL_CHECKPOINT_DIR_NAME = f\"gan_checkpoints_{SELECTED_DATASET_NAME}\"\n",
        "GDRIVE_CHECKPOINT_DIR_NAME = f\"gan_checkpoints_{SELECTED_DATASET_NAME}\" # Same name, but on Drive\n",
        "LOCAL_IMAGE_DIR = f\"gan_images_{SELECTED_DATASET_NAME}\"\n",
        "\n",
        "os.makedirs(LOCAL_CHECKPOINT_DIR_NAME, exist_ok=True)\n",
        "os.makedirs(LOCAL_IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "# Google Drive path (ensure Drive is mounted)\n",
        "gdrive_base_path = '/content/drive/My Drive/'\n",
        "gdrive_save_path = None\n",
        "if os.path.exists(gdrive_base_path): # Check if drive was mounted\n",
        "    gdrive_save_path = os.path.join(gdrive_base_path, GDRIVE_CHECKPOINT_DIR_NAME)\n",
        "    os.makedirs(gdrive_save_path, exist_ok=True)\n",
        "    print(f\"Checkpoints will be saved to Google Drive at: {gdrive_save_path}\")\n",
        "else:\n",
        "    print(\"Google Drive not mounted. Checkpoints will only be saved locally.\")\n",
        "\n",
        "\n",
        "# --- Initialize Models, Loss, and Optimizers ---\n",
        "generator = Generator(latent_dim, num_classes, img_shape_tuple)\n",
        "discriminator = BCEDiscriminator(num_classes, img_shape_tuple)\n",
        "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "adversarial_loss.to(device)\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7qRwIFYdp4o",
        "outputId": "d9e2e286-5cf7-4007-9730-270158a6cf3e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoints will be saved to Google Drive at: /content/drive/My Drive/gan_checkpoints_covid_ct\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  CHECKPOINT LOADING CONFIGURATION & FUNCTIONALITY\n",
        "# ==============================================================================\n",
        "LOAD_FROM_CHECKPOINT = True  # SET TO True TO ENABLE LOADING\n",
        "START_EPOCH = 0 # Will be updated if checkpoint is loaded\n",
        "\n",
        "def find_latest_checkpoint_epoch(checkpoint_dir, model_prefix):\n",
        "    \"\"\"Finds the latest epoch number for a given model prefix in a checkpoint directory.\"\"\"\n",
        "    latest_epoch = -1\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
        "        return latest_epoch # Directory doesn't exist\n",
        "\n",
        "    print(f\"Scanning for checkpoints in: {checkpoint_dir} with prefix: {model_prefix}\")\n",
        "    found_files = []\n",
        "    for f in os.listdir(checkpoint_dir):\n",
        "        if f.startswith(model_prefix) and f.endswith(\".pth\"):\n",
        "            # Corrected regular expression string:\n",
        "            match = re.search(r\"_epoch_(\\d+)\\.pth$\", f)\n",
        "            if match:\n",
        "                epoch_num = int(match.group(1))\n",
        "                found_files.append({'file': f, 'epoch': epoch_num})\n",
        "                if epoch_num > latest_epoch:\n",
        "                    latest_epoch = epoch_num\n",
        "    if not found_files:\n",
        "        print(\"No matching checkpoint files found.\")\n",
        "    else:\n",
        "        print(f\"Found {len(found_files)} potential checkpoint files. Latest epoch determined: {latest_epoch}\")\n",
        "    return latest_epoch\n",
        "\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if gdrive_save_path and os.path.exists(gdrive_save_path):\n",
        "        print(f\"Attempting to load latest checkpoints from Google Drive: {gdrive_save_path}\")\n",
        "        checkpoint_source_dir = gdrive_save_path\n",
        "    elif os.path.exists(LOCAL_CHECKPOINT_DIR_NAME): # Fallback to local if gdrive not available but local exists\n",
        "        print(f\"Google Drive path not available or does not exist. Attempting to load from local directory: {LOCAL_CHECKPOINT_DIR_NAME}\")\n",
        "        checkpoint_source_dir = LOCAL_CHECKPOINT_DIR_NAME\n",
        "    else:\n",
        "        print(\"Checkpoint loading enabled, but neither Google Drive path nor local checkpoint directory is available or specified.\")\n",
        "        checkpoint_source_dir = None\n",
        "\n",
        "    if checkpoint_source_dir:\n",
        "        latest_g_epoch = find_latest_checkpoint_epoch(checkpoint_source_dir, \"generator_epoch_\")\n",
        "        latest_d_epoch = find_latest_checkpoint_epoch(checkpoint_source_dir, \"discriminator_epoch_\")\n",
        "\n",
        "        # Determine the epoch to load from. Prefer a matched pair.\n",
        "        # If epochs are different, decide on a strategy (e.g., use the minimum of the two, or generator's, or error out)\n",
        "        # For this implementation, we'll try to load from the latest generator epoch and find a matching discriminator.\n",
        "        # If no matching discriminator, we'll use the discriminator's own latest epoch.\n",
        "\n",
        "        load_epoch_g = latest_g_epoch\n",
        "        load_epoch_d = latest_d_epoch\n",
        "\n",
        "        if load_epoch_g != -1: # Generator checkpoint(s) found\n",
        "            gen_checkpoint_path = os.path.join(checkpoint_source_dir, f\"generator_epoch_{load_epoch_g}.pth\")\n",
        "            try:\n",
        "                print(f\"Loading Generator from: {gen_checkpoint_path}\")\n",
        "                generator.load_state_dict(torch.load(gen_checkpoint_path, map_location=device))\n",
        "                print(f\"Successfully loaded Generator from epoch {load_epoch_g}.\")\n",
        "\n",
        "                # Try to load discriminator from the same epoch as generator for consistency\n",
        "                disc_checkpoint_path_matched = os.path.join(checkpoint_source_dir, f\"discriminator_epoch_{load_epoch_g}.pth\")\n",
        "                if os.path.exists(disc_checkpoint_path_matched):\n",
        "                    print(f\"Loading matching Discriminator from: {disc_checkpoint_path_matched}\")\n",
        "                    discriminator.load_state_dict(torch.load(disc_checkpoint_path_matched, map_location=device))\n",
        "                    print(f\"Successfully loaded Discriminator from epoch {load_epoch_g}.\")\n",
        "                    START_EPOCH = load_epoch_g + 1\n",
        "                elif load_epoch_d != -1: # No matched discriminator, but discriminator checkpoints exist\n",
        "                    disc_checkpoint_path_latest_d = os.path.join(checkpoint_source_dir, f\"discriminator_epoch_{load_epoch_d}.pth\")\n",
        "                    print(f\"No Discriminator checkpoint found for generator's epoch {load_epoch_g}. Trying latest D epoch: {load_epoch_d}\")\n",
        "                    print(f\"Loading Discriminator from: {disc_checkpoint_path_latest_d}\")\n",
        "                    discriminator.load_state_dict(torch.load(disc_checkpoint_path_latest_d, map_location=device))\n",
        "                    print(f\"Successfully loaded Discriminator from its latest epoch {load_epoch_d}.\")\n",
        "                    # Determine start epoch carefully if G and D epochs are different\n",
        "                    # Simplest is to start from max(load_epoch_g, load_epoch_d) + 1 or min(...) + 1\n",
        "                    # Or just use the generator's epoch as the primary reference for resuming\n",
        "                    START_EPOCH = load_epoch_g + 1\n",
        "                    if load_epoch_g != load_epoch_d:\n",
        "                        print(f\"Warning: Generator loaded from epoch {load_epoch_g}, Discriminator from epoch {load_epoch_d}. Resuming from epoch {START_EPOCH}.\")\n",
        "                else: # No discriminator checkpoints found at all\n",
        "                    print(f\"No Discriminator checkpoints found in {checkpoint_source_dir}. Discriminator will be initialized from scratch.\")\n",
        "                    START_EPOCH = load_epoch_g + 1 # Still resume based on generator\n",
        "\n",
        "                print(f\"Resuming training from epoch {START_EPOCH}.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading Generator checkpoint: {e}. Training will start from epoch 0.\")\n",
        "                START_EPOCH = 0\n",
        "\n",
        "        elif load_epoch_d != -1: # Only discriminator checkpoints found, no generator\n",
        "            print(\"Found Discriminator checkpoints but no Generator checkpoints.\")\n",
        "            print(\"Cannot resume training without a Generator checkpoint. Training will start from epoch 0.\")\n",
        "            START_EPOCH = 0\n",
        "        else:\n",
        "            print(f\"No suitable Generator or Discriminator checkpoints found in {checkpoint_source_dir}.\")\n",
        "            START_EPOCH = 0 # Ensure it's 0 if nothing is loaded\n",
        "\n",
        "    else: # checkpoint_source_dir was None\n",
        "        print(\"No checkpoint directory specified or found. Training will start from epoch 0.\")\n",
        "        START_EPOCH = 0\n",
        "else:\n",
        "    print(\"LOAD_FROM_CHECKPOINT is False. Training will start from epoch 0.\")\n",
        "    START_EPOCH = 0\n",
        "\n",
        "print(f\"Final START_EPOCH for training: {START_EPOCH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY6YqAjXltwg",
        "outputId": "395e832e-07f3-41cc-c931-12e4d42ffb98"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load latest checkpoints from Google Drive: /content/drive/My Drive/gan_checkpoints_covid_ct\n",
            "Scanning for checkpoints in: /content/drive/My Drive/gan_checkpoints_covid_ct with prefix: generator_epoch_\n",
            "No matching checkpoint files found.\n",
            "Scanning for checkpoints in: /content/drive/My Drive/gan_checkpoints_covid_ct with prefix: discriminator_epoch_\n",
            "No matching checkpoint files found.\n",
            "No suitable Generator or Discriminator checkpoints found in /content/drive/My Drive/gan_checkpoints_covid_ct.\n",
            "Final START_EPOCH for training: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(\"Starting Training Loop...\")\n",
        "save_interval = 10 # Save models and images every N epochs\n",
        "\n",
        "# Check if the dataloader was created successfully\n",
        "if main_dataloader is None:\n",
        "    print(\"FATAL: Dataloader not found or empty. Aborting training.\")\n",
        "    # Exit or handle as appropriate, script will end if this is None due to earlier logic\n",
        "else:\n",
        "    for epoch in range(START_EPOCH, epochs):\n",
        "        for i, (imgs, labels) in enumerate(main_dataloader):\n",
        "            real_imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Adversarial ground truths\n",
        "            valid = torch.ones(imgs.size(0), 1, requires_grad=False).to(device)\n",
        "            fake = torch.zeros(imgs.size(0), 1, requires_grad=False).to(device)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
        "            gen_labels = torch.randint(0, num_classes, (imgs.size(0),), device=device, dtype=torch.long)\n",
        "            gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "            validity_real = discriminator(real_imgs, labels)\n",
        "            d_real_loss = adversarial_loss(validity_real, valid)\n",
        "\n",
        "            validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
        "            d_fake_loss = adversarial_loss(validity_fake, fake)\n",
        "\n",
        "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate a fresh batch of fake images for the generator's turn\n",
        "            z_g = torch.randn(imgs.size(0), latent_dim).to(device)\n",
        "            gen_labels_g = torch.randint(0, num_classes, (imgs.size(0),), device=device, dtype=torch.long)\n",
        "            gen_imgs_g = generator(z_g, gen_labels_g)\n",
        "\n",
        "            validity = discriminator(gen_imgs_g, gen_labels_g)\n",
        "            g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            if i % 100 == 0: # Print progress\n",
        "                print(\n",
        "                    f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(main_dataloader)}] \"\n",
        "                    f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\"\n",
        "                )\n",
        "\n",
        "        # --- Save generated images and model checkpoints periodically ---\n",
        "        if epoch % save_interval == 0 or epoch == epochs -1 : # Save on interval or last epoch\n",
        "            print(f\"--- Saving models and images for epoch {epoch} ---\")\n",
        "\n",
        "            # Generate and save sample images\n",
        "            # Create a fixed noise and label set for consistent image generation across epochs\n",
        "            fixed_z_samples = num_classes * 5\n",
        "            fixed_z = torch.randn(fixed_z_samples, latent_dim).to(device)\n",
        "            fixed_sample_labels = torch.LongTensor(np.array([num for _ in range(5) for num in range(num_classes)])).to(device)\n",
        "\n",
        "            if len(fixed_sample_labels) > fixed_z_samples : # Should not happen with current logic\n",
        "                fixed_sample_labels = fixed_sample_labels[:fixed_z_samples]\n",
        "\n",
        "            gen_imgs_sample = generator(fixed_z, fixed_sample_labels).detach().cpu()\n",
        "\n",
        "            fig, axs = plt.subplots(num_classes, 5, figsize=(10, num_classes * 2 if num_classes > 1 else 3))\n",
        "            fig.suptitle(f\"Generated Images at Epoch {epoch} ({SELECTED_DATASET_NAME})\", fontsize=16)\n",
        "            for r_idx in range(num_classes):\n",
        "                for c_idx in range(5):\n",
        "                    img_index = r_idx * 5 + c_idx\n",
        "                    if img_index < len(gen_imgs_sample):\n",
        "                        # Transpose from (C, H, W) to (H, W, C) for display\n",
        "                        # Handle single channel (grayscale) or multi-channel\n",
        "                        img_display = gen_imgs_sample[img_index].permute(1, 2, 0) * 0.5 + 0.5 # Un-normalize\n",
        "                        current_ax = axs[r_idx, c_idx] if num_classes > 1 else axs[c_idx]\n",
        "\n",
        "                        if img_display.shape[2] == 1: # Grayscale\n",
        "                            current_ax.imshow(img_display.squeeze(), cmap='gray')\n",
        "                        else: # RGB\n",
        "                            current_ax.imshow(img_display)\n",
        "                        current_ax.set_title(f\"Class {r_idx}\")\n",
        "                        current_ax.axis('off')\n",
        "\n",
        "            image_save_path = os.path.join(LOCAL_IMAGE_DIR, f\"epoch_{epoch}.png\")\n",
        "            plt.savefig(image_save_path)\n",
        "            plt.close(fig) # Close the plot\n",
        "            print(f\"Sample images saved to {image_save_path}\")\n",
        "\n",
        "            # Save model checkpoints (locally and to Google Drive if available)\n",
        "            local_g_path = os.path.join(LOCAL_CHECKPOINT_DIR_NAME, f\"generator_epoch_{epoch}.pth\")\n",
        "            local_d_path = os.path.join(LOCAL_CHECKPOINT_DIR_NAME, f\"discriminator_epoch_{epoch}.pth\")\n",
        "            torch.save(generator.state_dict(), local_g_path)\n",
        "            torch.save(discriminator.state_dict(), local_d_path)\n",
        "            print(f\"Successfully saved Generator locally to: {local_g_path}\")\n",
        "            print(f\"Successfully saved Discriminator locally to: {local_d_path}\")\n",
        "\n",
        "            if gdrive_save_path:\n",
        "                gdrive_g_path = os.path.join(gdrive_save_path, f\"generator_epoch_{epoch}.pth\")\n",
        "                gdrive_d_path = os.path.join(gdrive_save_path, f\"discriminator_epoch_{epoch}.pth\")\n",
        "                try:\n",
        "                    torch.save(generator.state_dict(), gdrive_g_path)\n",
        "                    torch.save(discriminator.state_dict(), gdrive_d_path)\n",
        "                    print(f\"Successfully saved Generator to Google Drive: {gdrive_g_path}\")\n",
        "                    print(f\"Successfully saved Discriminator to Google Drive: {gdrive_d_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving checkpoints to Google Drive: {e}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # Final save (redundant if last epoch is caught by save_interval, but good as a fallback)\n",
        "    # This was in the original code, but typically the loop's `epoch == epochs -1` handles the final save.\n",
        "    # If keeping, ensure `epoch` variable is correctly reflecting the last completed epoch.\n",
        "    # For now, relying on the save within the loop for the last epoch."
      ],
      "metadata": {
        "id": "biNh-e9NVPHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c49e5579-e629-4284-efbd-0ce021a5bb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "[Epoch 0/100] [Batch 0/268] [D loss: 0.6987] [G loss: 0.7231]\n",
            "[Epoch 0/100] [Batch 100/268] [D loss: 0.6050] [G loss: 0.8805]\n",
            "[Epoch 0/100] [Batch 200/268] [D loss: 0.5998] [G loss: 1.0380]\n",
            "--- Saving models and images for epoch 0 ---\n",
            "Sample images saved to gan_images_covid_ct/epoch_0.png\n",
            "Successfully saved Generator locally to: gan_checkpoints_covid_ct/generator_epoch_0.pth\n",
            "Successfully saved Discriminator locally to: gan_checkpoints_covid_ct/discriminator_epoch_0.pth\n",
            "Successfully saved Generator to Google Drive: /content/drive/My Drive/gan_checkpoints_covid_ct/generator_epoch_0.pth\n",
            "Successfully saved Discriminator to Google Drive: /content/drive/My Drive/gan_checkpoints_covid_ct/discriminator_epoch_0.pth\n",
            "[Epoch 1/100] [Batch 0/268] [D loss: 0.7358] [G loss: 1.3538]\n",
            "[Epoch 1/100] [Batch 100/268] [D loss: 0.5973] [G loss: 0.9650]\n",
            "[Epoch 1/100] [Batch 200/268] [D loss: 0.6987] [G loss: 1.3842]\n",
            "[Epoch 2/100] [Batch 0/268] [D loss: 0.5804] [G loss: 1.0161]\n",
            "[Epoch 2/100] [Batch 100/268] [D loss: 0.5720] [G loss: 1.4442]\n",
            "[Epoch 2/100] [Batch 200/268] [D loss: 0.6473] [G loss: 0.3555]\n",
            "[Epoch 3/100] [Batch 0/268] [D loss: 0.6049] [G loss: 0.8668]\n",
            "[Epoch 3/100] [Batch 100/268] [D loss: 0.6368] [G loss: 1.6454]\n",
            "[Epoch 3/100] [Batch 200/268] [D loss: 0.5923] [G loss: 1.2900]\n",
            "[Epoch 4/100] [Batch 0/268] [D loss: 0.6851] [G loss: 1.2150]\n",
            "[Epoch 4/100] [Batch 100/268] [D loss: 0.7148] [G loss: 0.5877]\n",
            "[Epoch 4/100] [Batch 200/268] [D loss: 0.6232] [G loss: 0.9731]\n",
            "[Epoch 5/100] [Batch 0/268] [D loss: 0.5797] [G loss: 1.1290]\n",
            "[Epoch 5/100] [Batch 100/268] [D loss: 0.6774] [G loss: 0.7209]\n",
            "[Epoch 5/100] [Batch 200/268] [D loss: 0.6258] [G loss: 0.9366]\n",
            "[Epoch 6/100] [Batch 0/268] [D loss: 0.6472] [G loss: 1.3471]\n",
            "[Epoch 6/100] [Batch 100/268] [D loss: 0.6380] [G loss: 0.8304]\n",
            "[Epoch 6/100] [Batch 200/268] [D loss: 0.6998] [G loss: 0.8759]\n",
            "[Epoch 7/100] [Batch 0/268] [D loss: 0.6552] [G loss: 0.8032]\n",
            "[Epoch 7/100] [Batch 100/268] [D loss: 0.5420] [G loss: 0.9899]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat above steps for all datasets to create each expert\n"
      ],
      "metadata": {
        "id": "nzHNvLbl9H4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Sample Usage: Generating Images\n",
        "\n",
        "Once the model is trained, you can use the generator to create new, synthetic medical images."
      ],
      "metadata": {
        "id": "HmmlMiWlVTlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#  LOAD PRE-TRAINED GENERATOR AND GENERATE IMAGES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Configuration (Ensure these match the training configuration of the loaded model) ---\n",
        "# These would ideally be loaded from the same configuration used for training,\n",
        "# or ensure ACTIVE_DATASET_CONFIG is still in scope from previous cells.\n",
        "\n",
        "# If ACTIVE_DATASET_CONFIG is not in scope, you might need to redefine it or parts of it:\n",
        "# Example:\n",
        "# ACTIVE_DATASET_CONFIG = {\n",
        "#     \"num_classes\": 3,  # Replace with the actual number of classes for your saved model\n",
        "#     \"img_channels\": 1, # Replace with actual image channels\n",
        "#     \"img_size\": 64,    # Replace with actual image size\n",
        "#     # Add any other relevant keys if your Generator class uses them differently\n",
        "# }\n",
        "# latent_dim = 100 # Should be the same as during training\n",
        "\n",
        "# Ensure model class definition is available (e.g., from a previous cell or redefine here)\n",
        "# If not defined, you'd paste the Generator class definition here:\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, latent_dim, num_classes, img_shape_tuple):\n",
        "#         super(Generator, self).__init__()\n",
        "#         # ... (rest of the Generator class definition) ...\n",
        "#     def forward(self, noise, labels):\n",
        "#         # ... (rest of the forward pass) ...\n",
        "\n",
        "# --- Helper function to find the latest checkpoint (if needed) ---\n",
        "# This is the same function used during training checkpoint loading.\n",
        "def find_latest_epoch_in_dir(checkpoint_dir, model_prefix=\"generator_epoch_\"):\n",
        "    latest_epoch = -1\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
        "        return latest_epoch\n",
        "    for f in os.listdir(checkpoint_dir):\n",
        "        if f.startswith(model_prefix) and f.endswith(\".pth\"):\n",
        "            match = re.search(r\"_epoch_(\\d+)\\.pth$\", f) # Ensure r is before quote\n",
        "            if match:\n",
        "                epoch_num = int(match.group(1))\n",
        "                if epoch_num > latest_epoch:\n",
        "                    latest_epoch = epoch_num\n",
        "    return latest_epoch\n",
        "\n",
        "# --- Specify Checkpoint Path and Epoch to Load ---\n",
        "# Option 1: Load a specific epoch\n",
        "# LOAD_EPOCH = 90 # Example: if you want to load epoch 90\n",
        "# CHECKPOINT_BASE_DIR = gdrive_save_path # Or LOCAL_CHECKPOINT_DIR_NAME\n",
        "# generator_checkpoint_filename = f\"generator_epoch_{LOAD_EPOCH}.pth\"\n",
        "# generator_checkpoint_path = os.path.join(CHECKPOINT_BASE_DIR, generator_checkpoint_filename)\n",
        "\n",
        "# Option 2: Load the latest available generator epoch\n",
        "print(\"Attempting to load the latest generator checkpoint...\")\n",
        "# Determine which directory to search: Google Drive first, then local.\n",
        "checkpoint_load_dir = None\n",
        "if 'gdrive_save_path' in globals() and gdrive_save_path and os.path.exists(gdrive_save_path):\n",
        "    print(f\"Searching in Google Drive path: {gdrive_save_path}\")\n",
        "    checkpoint_load_dir = gdrive_save_path\n",
        "elif 'LOCAL_CHECKPOINT_DIR_NAME' in globals() and os.path.exists(LOCAL_CHECKPOINT_DIR_NAME):\n",
        "    print(f\"Google Drive path not found or specified. Searching in local path: {LOCAL_CHECKPOINT_DIR_NAME}\")\n",
        "    checkpoint_load_dir = LOCAL_CHECKPOINT_DIR_NAME\n",
        "else:\n",
        "    print(\"ERROR: Neither Google Drive checkpoint path nor local checkpoint directory is defined or found.\")\n",
        "    checkpoint_load_dir = None\n",
        "\n",
        "loaded_generator = None\n",
        "if checkpoint_load_dir:\n",
        "    latest_epoch_to_load = find_latest_epoch_in_dir(checkpoint_load_dir, \"generator_epoch_\")\n",
        "\n",
        "    if latest_epoch_to_load != -1:\n",
        "        generator_checkpoint_filename = f\"generator_epoch_{latest_epoch_to_load}.pth\"\n",
        "        generator_checkpoint_path = os.path.join(checkpoint_load_dir, generator_checkpoint_filename)\n",
        "        print(f\"Found latest generator checkpoint: {generator_checkpoint_path}\")\n",
        "\n",
        "        # --- Initialize Model ---\n",
        "        # Ensure these parameters match the saved model's training configuration.\n",
        "        # These should be accessible from previous cells (e.g., ACTIVE_DATASET_CONFIG)\n",
        "        current_num_classes = ACTIVE_DATASET_CONFIG['num_classes']\n",
        "        current_img_channels = ACTIVE_DATASET_CONFIG['img_channels']\n",
        "        current_img_size = ACTIVE_DATASET_CONFIG['img_size']\n",
        "        current_img_shape_tuple = (current_img_channels, current_img_size, current_img_size)\n",
        "        # latent_dim should also be available from training setup\n",
        "\n",
        "        # Instantiate the generator\n",
        "        loaded_generator = Generator(latent_dim, current_num_classes, current_img_shape_tuple)\n",
        "\n",
        "        # --- Load State Dictionary ---\n",
        "        try:\n",
        "            # Ensure `device` is defined (e.g., torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            loaded_generator.load_state_dict(torch.load(generator_checkpoint_path, map_location=device))\n",
        "            loaded_generator.to(device) # Move model to the device\n",
        "            loaded_generator.eval() # Set the model to evaluation mode (important for layers like BatchNorm, Dropout)\n",
        "            print(f\"Generator successfully loaded from {generator_checkpoint_path} and set to evaluation mode.\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"ERROR: Checkpoint file not found at {generator_checkpoint_path}\")\n",
        "            loaded_generator = None\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to load generator checkpoint: {e}\")\n",
        "            loaded_generator = None\n",
        "    else:\n",
        "        print(f\"No generator checkpoints found in {checkpoint_load_dir}.\")\n",
        "else:\n",
        "    print(\"No checkpoint directory available to load from.\")\n",
        "\n",
        "\n",
        "# --- Generate and Display Images (if generator was loaded) ---\n",
        "if loaded_generator:\n",
        "    num_images_to_generate_per_class = 5\n",
        "    total_images_to_generate = current_num_classes * num_images_to_generate_per_class\n",
        "\n",
        "    # Prepare noise and labels\n",
        "    # Use a fixed seed for reproducible generation if desired for testing\n",
        "    # torch.manual_seed(42)\n",
        "\n",
        "    # Generate random noise\n",
        "    z_generate = torch.randn(total_images_to_generate, latent_dim).to(device)\n",
        "\n",
        "    # Generate labels: (0,0,0,0,0, 1,1,1,1,1, 2,2,2,2,2, ... for each class)\n",
        "    gen_labels_list = []\n",
        "    for i in range(current_num_classes):\n",
        "        gen_labels_list.extend([i] * num_images_to_generate_per_class)\n",
        "\n",
        "    generated_labels = torch.LongTensor(gen_labels_list).to(device)\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad(): # No need to track gradients during generation\n",
        "        fake_images = loaded_generator(z_generate, generated_labels).detach().cpu()\n",
        "\n",
        "    print(f\"\\nGenerated {fake_images.shape[0]} images.\")\n",
        "\n",
        "    # --- Display Generated Images ---\n",
        "    fig_gen, axs_gen = plt.subplots(current_num_classes,\n",
        "                                    num_images_to_generate_per_class,\n",
        "                                    figsize=(num_images_to_generate_per_class * 2, current_num_classes * 2.2))\n",
        "    fig_gen.suptitle(f\"Images Generated by Loaded Model (Epoch {latest_epoch_to_load if 'latest_epoch_to_load' in locals() else 'Unknown'})\", fontsize=16)\n",
        "\n",
        "    for r_idx in range(current_num_classes):\n",
        "        for c_idx in range(num_images_to_generate_per_class):\n",
        "            img_index = r_idx * num_images_to_generate_per_class + c_idx\n",
        "            if img_index < len(fake_images):\n",
        "                # Un-normalize: images were normalized to [-1, 1], so (data * 0.5) + 0.5 brings them to [0, 1]\n",
        "                img_display = fake_images[img_index].permute(1, 2, 0) * 0.5 + 0.5\n",
        "\n",
        "                # Determine current axis, handling single row/column case for subplots\n",
        "                if current_num_classes == 1 and num_images_to_generate_per_class == 1:\n",
        "                    current_ax = axs_gen\n",
        "                elif current_num_classes == 1:\n",
        "                    current_ax = axs_gen[c_idx]\n",
        "                elif num_images_to_generate_per_class == 1:\n",
        "                    current_ax = axs_gen[r_idx]\n",
        "                else:\n",
        "                    current_ax = axs_gen[r_idx, c_idx]\n",
        "\n",
        "                if current_img_channels == 1: # Grayscale\n",
        "                    current_ax.imshow(img_display.squeeze(), cmap='gray')\n",
        "                else: # RGB\n",
        "                    current_ax.imshow(img_display.clip(0,1)) # Clip to ensure valid range for imshow\n",
        "\n",
        "                current_ax.set_title(f\"Class {r_idx}\")\n",
        "                current_ax.axis('off')\n",
        "            else: # Should not happen if total_images_to_generate is correct\n",
        "                if current_num_classes == 1 and num_images_to_generate_per_class == 1:\n",
        "                    axs_gen.axis('off')\n",
        "                elif current_num_classes == 1:\n",
        "                    axs_gen[c_idx].axis('off')\n",
        "                elif num_images_to_generate_per_class == 1:\n",
        "                    axs_gen[r_idx].axis('off')\n",
        "                else:\n",
        "                    axs_gen[r_idx, c_idx].axis('off')\n",
        "\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nGenerator not loaded. Cannot generate images.\")"
      ],
      "metadata": {
        "id": "ljRi1iVHVcEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps and Building an Ensemble\n",
        "\n",
        "This notebook provides a foundational cGAN. To build the full system you envision, you would:\n",
        "\n",
        "Expand to More Modalities: Adapt the dataset and model to handle MRI, CT, and X-ray images, increasing the num_classes parameter accordingly.\n",
        "\n",
        "Build an Ensemble:\n",
        "\n",
        "Mixture of Experts: You could train separate cGANs, one for each modality, and then use a classification model to decide which generator to use.\n",
        "\n",
        "Stacking: Train multiple different generative models (e.g., this cGAN, a VAE, and a diffusion model) and then use another neural network to combine their outputs.\n",
        "\n",
        "Improve the Classifier: For identifying the input scan type, you would train a dedicated, high-performance classification model on a labeled dataset of medical images.\n",
        "\n",
        "Incorporate Advanced Models: For higher-fidelity results, consider exploring more advanced generative models like diffusion models, which are the current state-of-the-art for image synthesis.\n",
        "\n",
        "This notebook should give you a solid starting point for your project. Good luck\n",
        "\n",
        "Sources\n",
        "```\n",
        "pyimagesearch.com\n",
        "kaggle.com\n",
        "ovhcloud.com\n",
        "github.com\n",
        "inria.fr\n",
        "```"
      ],
      "metadata": {
        "id": "jruFYsODTdgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here is a guide on how you can stack models to create a Mixture of Experts (MoE) model, complete with a Python code example using PyTorch.\n",
        "\n",
        "## Conceptual Overview of a Mixture of Experts (MoE) Model\n",
        "\n",
        "A Mixture of Experts model is a powerful ensemble technique where you have:\n",
        "\n",
        "Multiple \"Expert\" Models: Each expert is a neural network (or any other model) that is trained to become proficient in a specific subset of the problem space. For instance, in your medical imaging case, one expert might specialize in generating MRIs, another in CT scans, and a third in X-rays.\n",
        "\n",
        "A \"Gating Network\": This is a separate model that acts as a traffic controller. It takes the same input as the experts and decides how much to trust each expert for that specific input. It outputs a set of \"weights\" or \"probabilities\" that sum to 1.\n",
        "\n",
        "Final Output: The final output of the MoE model is a weighted sum of the outputs from all the expert models, with the weights determined by the gating network.\n",
        "\n",
        "This architecture allows the model to learn that different experts are better at different tasks, leading to better performance and more efficient use of parameters.\n",
        "\n",
        "Python Notebook: Stacking Models to Create an MoE\n",
        "\n",
        "Here’s a practical, step-by-step guide to building an MoE model. We'll create a simple example where different experts learn to model different parts of a synthetic dataset. You can then adapt this architecture to your more complex generative models."
      ],
      "metadata": {
        "id": "UUX14GrWaBrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "First, let's set up the environment and import the necessary libraries."
      ],
      "metadata": {
        "id": "I2eaXxD5aVq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sA6VWdQAab_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for creating a synthetic dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "2XKO8R3kagsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Defining the \"Expert\" Models\n",
        "\n",
        "Let's define a simple neural network that will serve as our expert. In a real-world application, these experts could be your complex generative models (like a GAN generator or a diffusion model). For this example, we'll use a simple feed-forward network."
      ],
      "metadata": {
        "id": "JaV7dko_aooN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "    \"\"\"A simple expert model.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Expert, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "BrYnvFvLaxi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Defining the \"Gating Network\"\n",
        "\n",
        "The gating network is responsible for deciding which expert to rely on for a given input. It will output a probability distribution over the experts. We use a Softmax activation to ensure the weights sum to 1."
      ],
      "metadata": {
        "id": "WE5XOdNUa7Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GatingNetwork(nn.Module):\n",
        "    \"\"\"A gating network that decides which expert to use.\"\"\"\n",
        "    def __init__(self, input_dim, num_experts):\n",
        "        super(GatingNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_experts),\n",
        "            nn.Softmax(dim=1)  # Output a probability distribution over experts\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "4tVEMeeIa-89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Stacking the Experts and Gating Network into an MoE Model\n",
        "\n",
        "Now, we'll combine the experts and the gating network into a single MoE model. This class will manage the forward pass, routing the input to the experts and combining their outputs based on the gating network's decisions."
      ],
      "metadata": {
        "id": "8z5O16udbDaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoEModel(nn.Module):\n",
        "    \"\"\"The main Mixture of Experts model.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, num_experts):\n",
        "        super(MoEModel, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Create the expert models\n",
        "        self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
        "\n",
        "        # Create the gating network\n",
        "        self.gating = GatingNetwork(input_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the weights from the gating network\n",
        "        gating_weights = self.gating(x)  # Shape: (batch_size, num_experts)\n",
        "\n",
        "        # Get the outputs from each expert\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        expert_outputs_tensor = torch.stack(expert_outputs, dim=2) # Shape: (batch_size, output_dim, num_experts)\n",
        "\n",
        "        # Weight the expert outputs\n",
        "        # We need to reshape the gating weights to multiply them with the expert outputs\n",
        "        gating_weights = gating_weights.unsqueeze(1) # Shape: (batch_size, 1, num_experts)\n",
        "\n",
        "        # The final output is the weighted sum of the expert outputs\n",
        "        weighted_expert_outputs = expert_outputs_tensor * gating_weights\n",
        "        final_output = torch.sum(weighted_expert_outputs, dim=2)\n",
        "\n",
        "        return final_output, gating_weights.squeeze(1)"
      ],
      "metadata": {
        "id": "F2XxuczZbJTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training and Usage\n",
        "\n",
        "Let's test our MoE model on a synthetic dataset. We'll create a dataset with 3 distinct clusters, which is a good scenario for an MoE with 3 experts."
      ],
      "metadata": {
        "id": "HYjzllmrbNBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 1. Create a Synthetic Dataset ---"
      ],
      "metadata": {
        "id": "AUZYvuwtbkYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll create a dataset with 3 centers, ideal for 3 experts\n",
        "X, y = make_blobs(n_samples=5000, centers=3, n_features=2, random_state=42, cluster_std=1.5)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.FloatTensor(y).view(-1, 1) # Reshape for regression-like loss\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "UZ1m2QdLboLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 2. Initialize and Train the Model ---"
      ],
      "metadata": {
        "id": "PwGN0dUNbt6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "num_experts = 3\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the MoE model\n",
        "moe_model = MoEModel(input_dim, output_dim, num_experts)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(moe_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "moe_model.to(device)\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = moe_model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "d9B1_J1mb2rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 3. Inspect the Gating Network's Decisions ---"
      ],
      "metadata": {
        "id": "O-wGsonrb78A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moe_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Take a few samples from the test set\n",
        "    sample_data = X_test[:15].to(device)\n",
        "    true_labels = y_test[:15]\n",
        "\n",
        "    # Get predictions and gating weights\n",
        "    predictions, gating_weights = moe_model(sample_data)\n",
        "\n",
        "    # For each sample, find out which expert was most influential\n",
        "    most_influential_expert = torch.argmax(gating_weights, dim=1)\n",
        "\n",
        "    print(\"\\n--- Gating Network Decisions ---\")\n",
        "    for i in range(len(sample_data)):\n",
        "        print(f\"Sample {i+1} (True Cluster: {int(true_labels[i].item())}): \"\n",
        "              f\"Predicted Value: {predictions[i].item():.2f}, \"\n",
        "              f\"Most Influential Expert: {most_influential_expert[i].item()}\")\n",
        "        print(f\"   Gating Weights: {[f'{w:.2f}' for w in gating_weights[i].cpu().numpy()]}\")"
      ],
      "metadata": {
        "id": "3sPo8txycAFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 4. Visualize Gating Decisions (Optional) ---"
      ],
      "metadata": {
        "id": "o1SOOCiLcC_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[link text](https://)plt.figure(figsize=(12, 5))\n",
        "plt.suptitle(\"Gating Network Specialization\", fontsize=16)\n",
        "\n",
        "# Get gating weights for the entire test set\n",
        "_, gating_weights_all = moe_model(X_test.to(device))\n",
        "most_influential_expert_all = torch.argmax(gating_weights_all, dim=1).cpu().numpy()\n",
        "\n",
        "# Plot for each expert\n",
        "for i in range(num_experts):\n",
        "    plt.subplot(1, num_experts, i + 1)\n",
        "    plt.title(f'Expert {i} Specialization')\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=(most_influential_expert_all == i), cmap='viridis', alpha=0.5)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8aY4VG13cKpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Adapt This to Your Medical Image Synthesis Task\n",
        "\n",
        "Replace the Experts: Your Expert class would be replaced by your generative models (e.g., a Generator from a GAN, or a DiffusionModel). The forward method of your expert would take a conditional input (like a noise vector and a class label) and output a generated image.\n",
        "\n",
        "Adapt the Gating Network: The GatingNetwork would take as input the condition you want to generate (e.g., a one-hot encoded vector representing \"MRI\", \"CT\", or \"X-ray\"). It would then output the weights for combining the outputs of your expert generators.\n",
        "\n",
        "Combine the Outputs: The most complex part is combining the outputs of generative models. Instead of a simple weighted sum of pixel values, you might want to perform this weighting in the latent space, or use a \"meta-learner\" (as in a stacking ensemble) to learn the best way to combine the generated images into a final, high-fidelity output.\n",
        "\n",
        "This MoE structure provides a very flexible and powerful way to build more sophisticated and specialized AI systems."
      ],
      "metadata": {
        "id": "zIaH0CjcZ4Ay"
      }
    }
  ]
}